---
title: "Lab 11 - Bootstrap"
author: "EE 375"
output: html_document
---


In the previous lab we use Maximum Likelihood to estimate parameters to the Farquhar, von Caemmerer, and Berry (1980) photosynthesis model [FvCB model] using leaf-level data on Aspen collected at the Niwot Ridge LTER site in Colorado. Today we're going to build on that example by looking at ways to estimate the uncertainty in the parameters.

### Task 1: Load the data, model, and MLE From Lab 10

The first step in today's analysis is just to reload the results from Lab 10. To complete today's lab you'll need to:

* Reload & subset the data
* Load the FvCB and lnLike functions
* Rerun the fit.  

You don't need to reproduce any of the other analyses and figures we did in Lab 10. Please cut-and-paste this code below so that this lab will knit successfully on its own.

```{r}
##' @name read.Licor
##' @author Mike Dietze
##' @param filename  name of the file to read
##' @param sep       file delimiter. defaults to tab
##' @param ...       optional arguments forwarded to read.table
read.Licor <- function(filename,sep="\t",...){
  fbase = sub(".txt","",tail(unlist(strsplit(filename,"/")),n=1))
  print(fbase)
  full = readLines(filename)
  ## remove meta=data
  start = grep(pattern="OPEN",full)
  skip = grep(pattern="STARTOFDATA",full)  
  for(i in length(start):1){
    full = full[-(start[i]:(skip[i]+1*(i>1)))] # +1 is to deal with second header
  }
  full = full[grep("\t",full)]  ## skip timestamp lines
  dat = read.table(textConnection(full),header = TRUE,blank.lines.skip=TRUE,sep=sep,...)
  fname=rep(fbase,nrow(dat))
  dat = as.data.frame(cbind(fname,dat))
  return(dat)
}

## Load the data
dat <- read.Licor("flux-course-3aci")

## grab the variables we need 
Ci = dat$Ci
Ca = dat$CO2R
Ao = dat$Photo

## sort the data in order for convinence
ord = order(Ci)
Ci = Ci[ord]
Ca = Ca[ord]
Ao = Ao[ord]


FvCB <- function(theta,Ci){
  Vcmax = theta[1]
  K  = theta[2]
  cp = theta[3]
  r  = theta[4]
  Anet = Vcmax*(Ci-cp)/(Ci+K) - r
  return(Anet)
}
param = c(13.0890097,139.0550059,91.0700930,-1.3636700,0.3352523)

lnLike <- function(theta,Ao,Ci){
  sigma = theta[5]
  lnL = sum(dnorm(Ao,FvCB(theta,Ci),sigma,log=TRUE))
  return(-lnL)
}
fit = optim(param,lnLike,Ao=Ao,Ci=Ci)
```


## Non-parameteric Bootstrap

The first method we will explore for estimating uncertainties is the non-parametric bootstrap. This method is based on resampling the original data, with replacement, in order to generate pseudodata sets. The original analysis is then performed on this replicate pseudodata. In this case we will be rerunning our Maximum Likelihood optimization in order to estimate the FvCB parameters for each data set. This process is repeated thousands of times in order to build up a distribution of parameter estimates.

The basic steps of the non-parametric bootstrap are to:

1. Generate a sample. In this case we're going to sample the row numbers so that we can preserve the relationship between our x and y variables (Ci and A)
2. Recalculate the statistics you are interested in. Here that's the MLE parameters
3. Save the results. Here we use a matrix, _theta.boot_, where each row is a different bootstrap estimate and each column is a different parameter. Thus looking down each column shows us all the parameter estimates.

```{r}
nboot = 2000
theta.boot = matrix(NA,nboot,5)
for(i in 1:nboot){
  samp <- sample(1:length(Ci),length(Ci),replace=TRUE) ### sample 
  out <- suppressWarnings(optim(fit$par,lnLike,Ao=Ao[samp],Ci=Ci[samp])) ### fit sample 
  theta.boot[i,] <- out$par  ## save results
  if(i%%100 == 0) print(i) ## counter. Comment out for knitting
}
colnames(theta.boot) = c("Vcmax","K","cp","r","sigma")  
```

### Task 2: Confidence Intervals

For each column in theta.boot, calculate a 95% Confidence interval. Do this by _applying_ the function _quantile_ to each column. _quantile_ is a basic summary statistic function, like mean, median, and var, except that it returns the specific quantiles you ask for in a data set. For example, quantile(x,0.5) would return the 50% (median) quantile of x. In this case you'll want to use the 0.025 and 0.975 quantiles in order to exclude the 2.5% most extreme values on each side of the distribution and keep the inner 95%.

```{r}
# Initialize df
summary_table = data.frame(matrix(NA, nrow = 3, ncol = 5))
colnames(summary_table) = c("Vcmax","K","cp","r","sigma") 
rownames(summary_table) = c("CI_lower", "CI_upper", "mean")

for (i in 1:5){
  # Variable Set Up
  title = colnames(theta.boot)[i]  # create a variable of what parameter we're looking at
  CI = quantile(theta.boot[,i],c(0.025,0.975)) # create a vector with the 2.5% and 97.5% CI
  MLE = mean(theta.boot[,i]) # calculate the mean of the data
  
  # Plotting
  hist(theta.boot[,i], main = title, xlab = title, breaks = 50)
  abline(v = MLE, col = "red", lwd = 2) # mean line
  abline(v = CI[1], col = "orange", lty = 2, lwd = 2) # lower CI bound
  abline(v = CI[2], col = "orange", lty = 2, lwd = 2) # upper CI bound
  
  # Legend
  legend("topright", legend = c("Mean", "CI"), col = c( "red", "orange"),
         lty = c(1, 2), lwd = c(2, 2))
  
  # Summary Tables
  summary_table[1,i] = CI[1]
  summary_table[2,i] = CI[2]
  summary_table[3,i] = MLE
}

```


### Task 3: Histograms

Plot histograms for each variable. Use the _abline_ function abline to add vertical lines for the Maximum Likelihood Estimate (MLE) (mean) and the Confidence Interval.

Based on the CI, which parameters are not significantly different from 0?

The only parameter not statistically significantly different than 0 is r. This is because the CI overlaps with 0.

### Task 4: Summary Table

Create a table that summarizes the MLE, standard deviation, and CI for each parameter. 

```{r}
# print summary table
print(summary_table)
```


### Task 5: Parameter correlations

Parameter estimates from the same model are frequently correlated with one another (e.g. if you increase one parameter you have to decrease another to get a similar prediction). It is important to check these correlations when fitting models, as it affects out ability to make inferences about individual parameters and to make predictions. Really strong correlations may indicate a model that is overparameterized (has too many parameters) as the data cannot tell the parameters apart. Frequently overparameterized models should be simplified (i.e. tested against simpler models that have fewer parameters.) 

The two ways that we assess parameter correlations are exactly the same as what we did when we assessed colinearity: we calculate the correlation matrix using _cor_ and we visually assess the correlations using scatterplots (e.g. _pairs_). Generate these plots and statistics and report which parameters are highly correlated with one another.

** Before proceeding to the next step, save the results so far (e.g. theta.boot) so you don't overwrite them in the next section. You will be asked to compare these results to those below**

```{r}
pairs(theta.boot)
```

K and Vcmax seem to be highly correlated along with cp and r.

## Parameteric Bootstrap

In lecture we learned that there were two alternatives for how to conduct the bootstrap: non-parametric and parameteric. The main difference with the parameteric is that the pseudodata is generated by simulating data _from the model_ rather than resampling from the data. The parameteric bootstrap makes a much stronger assumption about the model's equations and probability distributions being reasonable. However, the trade-off is that it is more robust under small sample sizes. The parametric approach is also easier to use to extrapolate to different sample sizes (e.g. power analyses). 

### Task 6: Generate Parameteric Bootstrap

Starting from the code for the non-parametic bootstrap (just above Task 2), modify this code to perform the parametric bootstrap. To do this you will have to change the 'sample' step into two steps

1. Draw a sample of input variables (in this case Ci) that is the _same size_ as the original data. I'd recommend drawing from a uniform distribution (_dunif_) over the range of Ci (e.g. 0 to 700).
2. Draw a sample of the output variable (in this case A). To do this you will need to run the model, FvCB, given the input (Ci) and the _best fit_ parameters (MLE estimates). You also need to include a residual error by simulating errors from the Normal distribution, again using the MLE estimate of the standard deviation, sigma, as the standard deviation in the Normal.

After completing these steps you will then have two simulated data sets (e.g. Ci.boot and A.boot) that you can then plug into optim. The rest of the bootstrap is exactly the same.

```{r}
nboot = 2000
n = length(Ci)
theta = fit$par
theta.boot = matrix(NA,nboot,5)
for(i in 1:nboot){
  Ciboot <- runif(n,0,700)
  Aboot <- rnorm(n,FvCB(theta,Ciboot), theta[5])
  out <- suppressWarnings(optim(fit$par, lnLike, Ao = Aboot, Ci = Ciboot))
  theta.boot[i, ] <- out$par
}
colnames(theta.boot) = c("Vcmax","K","cp","r","sigma")  
```


### Task 7: Parameteric Bootstrap Statistics

Repeat Tasks 2-5 for the Parameteric Bootstrap outputs

```{r}
# Initialize df
summary_table = data.frame(matrix(NA, nrow = 3, ncol = 5))
colnames(summary_table) = c("Vcmax","K","cp","r","sigma") 
rownames(summary_table) = c("CI_lower", "CI_upper", "mean")

for (i in 1:5){
  # Variable Set Up
  title = colnames(theta.boot)[i]  # create a variable of what parameter we're looking at
  CI = quantile(theta.boot[,i],c(0.025,0.975)) # create a vector with the 2.5% and 97.5% CI
  MLE = mean(theta.boot[,i]) # calculate the mean of the data
  
  # Plotting
  hist(theta.boot[,i], main = title, xlab = title)
  abline(v = MLE, col = "red", lwd = 2) # mean line
  abline(v = CI[1], col = "orange", lty = 2, lwd = 2) # lower CI bound
  abline(v = CI[2], col = "orange", lty = 2, lwd = 2) # upper CI bound
  
  # Legend
  legend("topright", legend = c("Mean", "CI"), col = c( "red", "orange"),
         lty = c(1, 2), lwd = c(2, 2))
  
  # Summary Tables
  summary_table[1,i] = CI[1]
  summary_table[2,i] = CI[2]
  summary_table[3,i] = MLE
}

# print summary table
print(summary_table)

pairs(theta.boot)
```

Vcmax is the correlated with K, cp, and r. K is correlated with cp. r is correlated with cp.

### Task 8: Comparison

Compare your results from the two approaches

The parametric bootstrap had more correlations than the non-parametric bootstrap. This makes sense as the psuedo data all has the same distribution in a parametric bootstrap. The mean values for Vcmax, K, cp, r, and sigma are higher for some and lower for some between the two methods but are all roughly the same

### Extra Credit: Power Analysis

To explore what would happen if you could increase your sample size, rerun the parametric bootstrap for a range of **increasing** sample sizes. Specifically if _n_ is the size of the original data set, run the analysis at sample sizes of n, 2n, 4n, 8n, 16n, and 32n.  

Next, for the variable *r* specifically, plot the confidence interval as a function of sample size. In plot set log='x' to put the x-axis on a log scale (since we varied n by factors of 2)

What sample size would be required to be able to distinguish *r* from zero?

```{r}
n_vector = c(Ci,2*Ci,4*Ci,8*Ci,16*Ci,32*Ci)

for(ii in 1:6){
  
  nboot = 2000
  n = n_vector[ii]
  theta = fit$par
  theta.boot = matrix(NA,nboot,5)
  
  for(i in 1:nboot){
    Ciboot <- runif(n,0,700)
    Aboot <- rnorm(n,FvCB(theta,Ciboot), theta[5])
    out <- suppressWarnings(optim(fit$par, lnLike, Ao = Aboot, Ci = Ciboot))
    theta.boot[i, ] <- out$par
  }

colnames(theta.boot) = c("Vcmax","K","cp","r","sigma")  

# Initialize df
summary_table = data.frame(matrix(NA, nrow = 3, ncol = 5))
colnames(summary_table) = c("Vcmax","K","cp","r","sigma") 
rownames(summary_table) = c("CI_lower", "CI_upper", "mean")

for (i in 1:5){
  # Variable Set Up
  title = colnames(theta.boot)[i]  # create a variable of what parameter we're looking at
  CI = quantile(theta.boot[,i],c(0.025,0.975)) # create a vector with the 2.5% and 97.5% CI
  MLE = mean(theta.boot[,i]) # calculate the mean of the data
  
  # Plotting
  hist(theta.boot[,i], main = title, xlab = title)
  abline(v = MLE, col = "red", lwd = 2) # mean line
  abline(v = CI[1], col = "orange", lty = 2, lwd = 2) # lower CI bound
  abline(v = CI[2], col = "orange", lty = 2, lwd = 2) # upper CI bound
  
  # Legend
  legend("topright", legend = c("Mean", "CI"), col = c( "red", "orange"),
         lty = c(1, 2), lwd = c(2, 2))
  
  # Summary Tables
  summary_table[1,i] = CI[1]
  summary_table[2,i] = CI[2]
  summary_table[3,i] = MLE
}

# print summary table
print(summary_table)

pairs(theta.boot)

}
```

